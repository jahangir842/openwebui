Bootstrap: docker
From: ollama/ollama:latest

%post
    # Install necessary dependencies
    apt-get update && apt-get install -y --no-install-recommends \
        curl \
        ca-certificates \
        nvidia-cuda-toolkit \
        nvidia-cudnn \
        && rm -rf /var/lib/apt/lists/*
    
    # Create a custom startup script that properly handles the bind mount
    cat > /usr/local/bin/start-ollama.sh << 'EOF'
#!/bin/bash
set -e

# Create necessary directories in the bound volume
mkdir -p /mnt/data/models
mkdir -p /mnt/data/config

# Set environment variables for Ollama
export OLLAMA_MODELS=/mnt/data/models
export OLLAMA_HOST=${OLLAMA_HOST:-"0.0.0.0:11435"}
export OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-"1h"}

# A100 optimizations
export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-"0,1,2,3"}
export CUDA_DEVICE_MAX_CONNECTIONS=${CUDA_DEVICE_MAX_CONNECTIONS:-"1"}
export OMP_NUM_THREADS=${OMP_NUM_THREADS:-"8"}
export LLAMA_CUBLAS=${LLAMA_CUBLAS:-"1"}
export GPU_LAYERS=${GPU_LAYERS:-"100"}
export GPU_BATCH_SIZE=${GPU_BATCH_SIZE:-"2048"}

echo "Starting Ollama server with:"
echo "- MODELS_DIR: $OLLAMA_MODELS"
echo "- HOST: $OLLAMA_HOST"
echo "- CUDA devices: $CUDA_VISIBLE_DEVICES"

# Start Ollama server
ollama serve
EOF

    # Make it executable
    chmod +x /usr/local/bin/start-ollama.sh

%environment
    # Default environment variables
    export OLLAMA_HOST=0.0.0.0:11435
    export OLLAMA_MODELS=/mnt/data/models
    export OLLAMA_KEEP_ALIVE=1h
    
    # CUDA configuration
    export CUDA_VISIBLE_DEVICES=0,1,2,3
    
    # A100 optimizations
    export CUDA_DEVICE_MAX_CONNECTIONS=1
    export OMP_NUM_THREADS=8
    export LLAMA_CUBLAS=1
    export GPU_LAYERS=100
    export GPU_BATCH_SIZE=2048

%startscript
    /usr/local/bin/start-ollama.sh

%runscript
    /usr/local/bin/start-ollama.sh

%help
    Ollama Singularity Container optimized for NVIDIA A100 GPUs
    
    === REQUIRED COMMAND ===
    You MUST provide a bind mount for the data directory:
    
    singularity run --nv -B /your/local/path:/mnt/data ollama.sif
    
    === EXAMPLES ===
    
    Run with basic configuration:
    singularity run --nv -B /home/user/ollama-data:/mnt/data ollama.sif
    
    Run with custom port:
    OLLAMA_HOST=0.0.0.0:11436 singularity run --nv -B /home/user/ollama-data:/mnt/data ollama.sif
    
    Pull a model (from another terminal while Ollama is running):
    singularity exec --nv ollama.sif ollama pull llama3
    
    === CONNECTION INFO ===
    By default, this container uses port 11435 instead of the standard 11434.
    
    When connecting with Open WebUI, set the Ollama API endpoint to:
    http://localhost:11435

%labels
    Author "Custom Ollama Container"
    Version "latest"
    A100_optimized "yes"
    GPU_count "4"
